This is the minimal implementation of Multi-Layer Perceptron with single hidden layer.

The activation function in the hidden layer is sigmoid function.

The activation function in the output layer is softmax function.

The network is updated with gradient descent. 

In each epoch, all the training data (60000) is used to generate one update value.

Momentum is not used in this implementation.

bp.py is the leayer-based Object-oriented version but not working correctly right now. Needs to be updated.


# TODO (if there is time)
# Run uniform initialization 0.1, 0.5, 1.0 several times?
